"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.default = void 0;

var _Asset = _interopRequireDefault(require("./Asset"));

var _path = _interopRequireDefault(require("path"));

var _clone = _interopRequireDefault(require("clone"));

var _md = require("@parcel/utils/src/md5");

var _cache = _interopRequireDefault(require("@parcel/cache"));

var _fs = require("fs");

var _collection = require("@parcel/utils/src/collection");

var _Config = _interopRequireDefault(require("./Config"));

var _ReporterRunner = require("./ReporterRunner");

var _TapStream = _interopRequireDefault(require("@parcel/utils/src/TapStream"));

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _defineProperty(obj, key, value) { if (key in obj) { Object.defineProperty(obj, key, { value: value, enumerable: true, configurable: true, writable: true }); } else { obj[key] = value; } return obj; }

const BUFFER_LIMIT = 5000000; // 5mb

class TransformerRunner {
  constructor(opts) {
    _defineProperty(this, "options", void 0);

    _defineProperty(this, "config", void 0);

    this.options = opts.options;
    this.config = opts.config;
  }

  async transform(req) {
    (0, _ReporterRunner.report)({
      type: 'buildProgress',
      phase: 'transforming',
      request: req
    }); // If a cache entry matches, no need to transform.

    let cacheEntry;

    if (this.options.cache !== false && req.code == null) {
      cacheEntry = await _cache.default.get(reqCacheKey(req));
    }

    let {
      content,
      size,
      hash
    } = await summarizeRequest(req);

    if (cacheEntry && cacheEntry.hash === hash && (await checkCachedAssets(cacheEntry.assets))) {
      return cacheEntry;
    }

    let input = new _Asset.default({
      // If the transformer request passed code rather than a filename,
      // use a hash as the base for the id to ensure it is unique.
      idBase: req.code ? hash : req.filePath,
      filePath: req.filePath,
      type: _path.default.extname(req.filePath).slice(1),
      ast: null,
      content,
      hash,
      env: req.env,
      stats: {
        time: 0,
        size
      },
      sideEffects: req.sideEffects
    });
    let pipeline = await this.config.getTransformers(req.filePath);
    let {
      assets,
      initialAssets
    } = await this.runPipeline(input, pipeline, cacheEntry);
    cacheEntry = {
      filePath: req.filePath,
      env: req.env,
      hash,
      assets,
      initialAssets
    };
    await Promise.all((0, _collection.unique)(assets, initialAssets || []).map(asset => asset.commit()));
    await _cache.default.set(reqCacheKey(req), cacheEntry);
    return cacheEntry;
  }

  async runPipeline(input, pipeline, cacheEntry, previousGenerate) {
    // Run the first transformer in the pipeline.
    let {
      results,
      generate,
      postProcess
    } = await this.runTransform(input, pipeline[0], previousGenerate);
    let assets = [];

    for (let result of results) {
      let asset = input.createChildAsset(result); // Check if any of the cached assets match the result.

      if (cacheEntry) {
        let cachedAssets = (cacheEntry.initialAssets || cacheEntry.assets).filter(child => child.hash && child.hash === asset.hash);

        if (cachedAssets.length > 0 && (await checkCachedAssets(cachedAssets))) {
          assets = assets.concat(cachedAssets);
          continue;
        }
      } // If the generated asset has the same type as the input...
      // TODO: this is incorrect since multiple file types could map to the same pipeline. need to compare the pipelines.


      if (result.type === input.type) {
        // If we have reached the last transform in the pipeline, then we are done.
        if (pipeline.length === 1) {
          assets.push((await finalize(asset, generate)));
        } else {
          // Recursively run the remaining transforms in the pipeline.
          let nextPipelineResult = await this.runPipeline(asset, pipeline.slice(1), null, generate);
          assets = assets.concat(nextPipelineResult.assets);
        }
      } else {
        // Jump to a different pipeline for the generated asset.
        let nextFilePath = input.filePath.slice(0, -_path.default.extname(input.filePath).length) + '.' + result.type;
        let nextPipelineResult = await this.runPipeline(asset, (await this.config.getTransformers(nextFilePath)), null, generate);
        assets = assets.concat(nextPipelineResult.assets);
      }
    } // If the transformer has a postProcess function, execute that with the result of the pipeline.


    let finalAssets = await postProcess((0, _clone.default)(assets));
    return {
      assets: finalAssets || assets,
      initialAssets: finalAssets ? assets : null
    };
  }

  async runTransform(input, transformer, previousGenerate) {
    // Load config for the transformer.
    let config = null;

    if (transformer.getConfig) {
      config = await transformer.getConfig(input, this.options);
    } // If an ast exists on the input, but we cannot reuse it,
    // use the previous transform to generate code that we can re-parse.


    if (input.ast && (!transformer.canReuseAST || !transformer.canReuseAST(input.ast, this.options)) && previousGenerate) {
      let output = await previousGenerate(input);
      input.content = output.code;
      input.ast = null;
    } // Parse if there is no AST available from a previous transform.


    if (!input.ast && transformer.parse) {
      input.ast = await transformer.parse(input, config, this.options);
    } // Transform.


    let results = await transformer.transform(input, config, this.options); // Create a generate function that can be called later to lazily generate

    let generate = async input => {
      if (transformer.generate) {
        return transformer.generate(input, config, this.options);
      }

      throw new Error('Asset has an AST but no generate method is available on the transform');
    }; // Create a postProcess function that can be called later


    let postProcess = async assets => {
      if (transformer.postProcess) {
        let results = await transformer.postProcess(assets, config, this.options);
        return Promise.all(results.map(result => input.createChildAsset(result)));
      }

      return null;
    }; // $FlowFixMe


    return {
      results,
      generate,
      postProcess
    };
  }

}

exports.default = TransformerRunner;

_defineProperty(TransformerRunner, "__exportSpecifier", "@parcel/core/lib/TransformerRunner.js");

async function finalize(asset, generate) {
  if (asset.ast && generate) {
    let result = await generate(asset);
    asset.content = result.code;
    asset.map = result.map;
  }

  return asset;
}

async function checkCachedAssets(assets) {
  let results = await Promise.all(assets.map(asset => checkConnectedFiles(asset.getConnectedFiles())));
  return results.every(Boolean);
}

async function checkConnectedFiles(files) {
  let hashes = await Promise.all(files.map(file => (0, _md.md5FromFilePath)(file.filePath)));
  return files.every((file, index) => file.hash === hashes[index]);
}

function reqCacheKey(req) {
  return (0, _md.md5FromString)(req.filePath + JSON.stringify(req.env));
}

async function summarizeRequest(req) {
  let code = req.code;
  let content;
  let hash;
  let size;

  if (code == null) {
    // As an optimization for the common case of source code, while we read in
    // data to compute its md5 and size, buffer its contents in memory.
    // This avoids reading the data now, and then again during transformation.
    // If it exceeds BUFFER_LIMIT, throw it out and replace it with a stream to
    // lazily read it at a later point.
    content = Buffer.from([]);
    size = 0;
    hash = await (0, _md.md5FromReadableStream)((0, _fs.createReadStream)(req.filePath).pipe(new _TapStream.default(buf => {
      if (content instanceof Buffer) {
        size += buf.length;

        if (size > BUFFER_LIMIT) {
          // if buffering this content would put this over BUFFER_LIMIT, replace
          // it with a stream
          content = (0, _fs.createReadStream)(req.filePath);
        } else {
          content = Buffer.concat([content, buf]);
        }
      }
    })));
  } else {
    content = code;
    hash = (0, _md.md5FromString)(code);
    size = Buffer.from(code).length;
  }

  return {
    content,
    hash,
    size
  };
}